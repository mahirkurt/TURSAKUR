#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TURSAKUR - TR Hastane Ãœniversite Hastaneleri Scraper
https://www.trhastane.com/ kaynaÄŸÄ±ndan Ã¼niversite hastanesi verilerini Ã§eker
"""

import requests
import json
import os
import time
import logging
import hashlib
from datetime import datetime
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict
from bs4 import BeautifulSoup, Tag
import unicodedata
import re

# Logging konfigÃ¼rasyonu
os.makedirs('logs', exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/fetch_trhastane_universite.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class UniversiteHastanesi:
    """Ãœniversite hastanesi veri yapÄ±sÄ±"""
    kurum_id: str
    kurum_adi: str
    kurum_tipi: str
    il_kodu: int
    il_adi: str
    ilce_adi: str
    adres: str
    telefon: str
    koordinat_lat: Optional[float]
    koordinat_lon: Optional[float]
    web_sitesi: str
    veri_kaynagi: str
    son_guncelleme: str
    trhastane_url: Optional[str] = None
    universite_adi: Optional[str] = None

class TRHastaneUniversiteScraper:
    """TR Hastane Ã¼niversite hastaneleri scraper sÄ±nÄ±fÄ±"""
    
    def __init__(self):
        """Scraper'Ä± baÅŸlat"""
        self.base_url = "https://www.trhastane.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'tr-TR,tr;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        self.hospitals = []
        self.retry_count = 3
        self.delay_range = (1, 3)
        
        # Ä°l kodlarÄ± haritasÄ± (TÃ¼rkiye)
        self.il_kodlari = {
            'adana': 1, 'adÄ±yaman': 2, 'afyonkarahisar': 3, 'aÄŸrÄ±': 4, 'amasya': 5,
            'ankara': 6, 'antalya': 7, 'artvin': 8, 'aydÄ±n': 9, 'balÄ±kesir': 10,
            'bilecik': 11, 'bingÃ¶l': 12, 'bitlis': 13, 'bolu': 14, 'burdur': 15,
            'bursa': 16, 'Ã§anakkale': 17, 'Ã§ankÄ±rÄ±': 18, 'Ã§orum': 19, 'denizli': 20,
            'diyarbakÄ±r': 21, 'edirne': 22, 'elazÄ±ÄŸ': 23, 'erzincan': 24, 'erzurum': 25,
            'eskiÅŸehir': 26, 'gaziantep': 27, 'giresun': 28, 'gÃ¼mÃ¼ÅŸhane': 29, 'hakkari': 30,
            'hatay': 31, 'isparta': 32, 'mersin': 33, 'istanbul': 34, 'izmir': 35,
            'kars': 36, 'kastamonu': 37, 'kayseri': 38, 'kÄ±rklareli': 39, 'kÄ±rÅŸehir': 40,
            'kocaeli': 41, 'konya': 42, 'kÃ¼tahya': 43, 'malatya': 44, 'manisa': 45,
            'kahramanmaraÅŸ': 46, 'mardin': 47, 'muÄŸla': 48, 'muÅŸ': 49, 'nevÅŸehir': 50,
            'niÄŸde': 51, 'ordu': 52, 'rize': 53, 'sakarya': 54, 'samsun': 55,
            'siirt': 56, 'sinop': 57, 'sivas': 58, 'tekirdaÄŸ': 59, 'tokat': 60,
            'trabzon': 61, 'tunceli': 62, 'ÅŸanlÄ±urfa': 63, 'uÅŸak': 64, 'van': 65,
            'yozgat': 66, 'zonguldak': 67, 'aksaray': 68, 'bayburt': 69, 'karaman': 70,
            'kÄ±rÄ±kkale': 71, 'batman': 72, 'ÅŸÄ±rnak': 73, 'bartÄ±n': 74, 'ardahan': 75,
            'iÄŸdÄ±r': 76, 'yalova': 77, 'karabÃ¼k': 78, 'kilis': 79, 'osmaniye': 80,
            'dÃ¼zce': 81
        }
    
    def normalize_text(self, text: str) -> str:
        """Metni normalize et"""
        if not text:
            return ""
        
        # Unicode normalize
        text = unicodedata.normalize('NFKD', str(text))
        
        # Fazla boÅŸluklarÄ± temizle
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        return text
    
    def get_il_kodu(self, il_adi: str) -> int:
        """Ä°l adÄ±ndan il kodunu al"""
        if not il_adi:
            return 0
        
        normalized_il = self.normalize_text(il_adi.lower())
        normalized_il = re.sub(r'[^a-zÃ§ÄŸÄ±Ã¶ÅŸÃ¼]', '', normalized_il)
        
        return self.il_kodlari.get(normalized_il, 0)
    
    def generate_kurum_id(self, hospital_data: Dict[str, Any]) -> str:
        """Benzersiz kurum ID oluÅŸtur"""
        # Hastane adÄ± + il kodu + tip kombinasyonu ile hash
        unique_string = f"{hospital_data['kurum_adi']}-{hospital_data['il_kodu']}-UNIVERSITE"
        hash_obj = hashlib.md5(unique_string.encode('utf-8'))
        hash_hex = hash_obj.hexdigest()[:4].upper()
        
        return f"TR-{hospital_data['il_kodu']:02d}-UH-{hash_hex}"
    
    def make_request(self, url: str, max_retries: int = 3) -> Optional[requests.Response]:
        """GÃ¼venli HTTP isteÄŸi yap"""
        for attempt in range(max_retries):
            try:
                logger.debug(f"Ä°stek yapÄ±lÄ±yor: {url} (Deneme {attempt + 1})")
                
                response = self.session.get(url, timeout=30)
                
                if response.status_code == 200:
                    return response
                elif response.status_code == 404:
                    logger.warning(f"Sayfa bulunamadÄ±: {url}")
                    return None
                else:
                    logger.warning(f"HTTP {response.status_code}: {url}")
                
            except requests.exceptions.RequestException as e:
                logger.warning(f"Ä°stek hatasÄ± (Deneme {attempt + 1}): {e}")
                
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # Exponential backoff
        
        logger.error(f"Maksimum deneme aÅŸÄ±ldÄ±: {url}")
        return None
    
    def find_university_hospitals_urls(self) -> List[str]:
        """Ãœniversite hastanelerinin URL'lerini bul"""
        logger.info("Ãœniversite hastaneleri sayfa URL'leri aranÄ±yor...")
        
        urls = []
        
        # Ana kategoriler ve olasÄ± URL patternleri
        search_patterns = [
            "/universite-hastaneleri",
            "/tip-fakultesi-hastaneleri",
            "/egitim-arastirma-hastaneleri",
            "/saglik-kurumu/universite-hastanesi",
            "/kategori/universite-hastanesi"
        ]
        
        # Ana sayfa ve kategorileri kontrol et
        main_response = self.make_request(self.base_url)
        if main_response:
            soup = BeautifulSoup(main_response.content, 'html.parser')
            
            # Ãœniversite hastanesi ile ilgili linkleri bul
            for link in soup.find_all('a', href=True):
                href = link['href']
                link_text = self.normalize_text(link.get_text())
                
                # Ãœniversite hastanesi iÃ§eren linkler
                if any(keyword in link_text.lower() for keyword in 
                       ['Ã¼niversite', 'tÄ±p fakÃ¼ltesi', 'eÄŸitim araÅŸtÄ±rma']):
                    
                    if href.startswith('/'):
                        urls.append(f"{self.base_url}{href}")
                    elif href.startswith('http'):
                        urls.append(href)
        
        # Direkt URL denemeleri
        for pattern in search_patterns:
            test_url = f"{self.base_url}{pattern}"
            response = self.make_request(test_url)
            if response and response.status_code == 200:
                urls.append(test_url)
        
        # Sitemap kontrolÃ¼
        sitemap_url = f"{self.base_url}/sitemap.xml"
        sitemap_response = self.make_request(sitemap_url)
        if sitemap_response:
            try:
                from xml.etree import ElementTree as ET
                root = ET.fromstring(sitemap_response.content)
                
                for url_elem in root.findall('.//{http://www.sitemaps.org/schemas/sitemap/0.9}url'):
                    loc_elem = url_elem.find('{http://www.sitemaps.org/schemas/sitemap/0.9}loc')
                    if loc_elem is not None:
                        url = loc_elem.text
                        if any(keyword in url.lower() for keyword in 
                               ['universite', 'tip-fakultesi', 'egitim']):
                            urls.append(url)
            except Exception as e:
                logger.debug(f"Sitemap parse hatasÄ±: {e}")
        
        # Duplicate'larÄ± kaldÄ±r
        unique_urls = list(set(urls))
        logger.info(f"{len(unique_urls)} adet potansiyel URL bulundu")
        
        return unique_urls
    
    def extract_hospital_from_page(self, url: str) -> List[Dict[str, Any]]:
        """Tek sayfadan hastane bilgilerini Ã§Ä±kar"""
        hospitals = []
        
        response = self.make_request(url)
        if not response:
            return hospitals
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # FarklÄ± HTML yapÄ±larÄ± iÃ§in selectors
        hospital_selectors = [
            '.hospital-item',
            '.hastane-item',
            '.hospital-card',
            '.content-item',
            '.hospital-list-item',
            'article',
            '.entry-content'
        ]
        
        hospital_elements = []
        for selector in hospital_selectors:
            elements = soup.select(selector)
            if elements:
                hospital_elements = elements
                break
        
        # EÄŸer spesifik selector yoksa, div'leri ara
        if not hospital_elements:
            # BaÅŸlÄ±k iÃ§inde "hastane" geÃ§en div'leri bul
            for div in soup.find_all('div'):
                if div.find(['h1', 'h2', 'h3', 'h4']):
                    header = div.find(['h1', 'h2', 'h3', 'h4'])
                    if header and 'hastane' in header.get_text().lower():
                        hospital_elements.append(div)
        
        for element in hospital_elements:
            try:
                hospital_data = self.extract_hospital_data(element, url)
                if hospital_data and self.is_university_hospital(hospital_data):
                    hospitals.append(hospital_data)
            except Exception as e:
                logger.debug(f"Hastane Ã§Ä±karma hatasÄ±: {e}")
        
        return hospitals
    
    def extract_hospital_data(self, element: Tag, source_url: str) -> Optional[Dict[str, Any]]:
        """HTML elementinden hastane verisi Ã§Ä±kar"""
        try:
            # Hastane adÄ±
            name_element = element.find(['h1', 'h2', 'h3', 'h4', 'a', '.title', '.name'])
            if not name_element:
                return None
            
            kurum_adi = self.normalize_text(name_element.get_text())
            if not kurum_adi or len(kurum_adi) < 5:
                return None
            
            # Adres bilgisi
            adres = ""
            address_keywords = ['adres', 'address', 'konum', 'location']
            for keyword in address_keywords:
                addr_elem = element.find(text=re.compile(keyword, re.IGNORECASE))
                if addr_elem:
                    parent = addr_elem.find_parent()
                    if parent:
                        adres = self.normalize_text(parent.get_text())
                        break
            
            # Ä°l/ilÃ§e Ã§Ä±karÄ±mÄ±
            il_adi, ilce_adi = self.extract_location_from_text(kurum_adi + " " + adres)
            
            # Telefon
            telefon = ""
            phone_pattern = re.compile(r'(\+90\s?)?(\d{3})\s?(\d{3})\s?(\d{2})\s?(\d{2})')
            phone_match = phone_pattern.search(element.get_text())
            if phone_match:
                telefon = phone_match.group(0)
            
            # Web sitesi
            web_sitesi = ""
            link_elem = element.find('a', href=True)
            if link_elem and 'http' in link_elem['href']:
                web_sitesi = link_elem['href']
            
            # Ãœniversite adÄ± Ã§Ä±kar
            universite_adi = self.extract_university_name(kurum_adi)
            
            hospital_data = {
                'kurum_adi': kurum_adi,
                'kurum_tipi': 'Ãœniversite Hastanesi',
                'il_adi': il_adi,
                'il_kodu': self.get_il_kodu(il_adi),
                'ilce_adi': ilce_adi,
                'adres': adres,
                'telefon': telefon,
                'koordinat_lat': None,
                'koordinat_lon': None,
                'web_sitesi': web_sitesi,
                'veri_kaynagi': 'TR Hastane - Ãœniversite Hastaneleri',
                'son_guncelleme': datetime.now().strftime('%Y-%m-%d'),
                'trhastane_url': source_url,
                'universite_adi': universite_adi
            }
            
            # ID oluÅŸtur
            hospital_data['kurum_id'] = self.generate_kurum_id(hospital_data)
            
            return hospital_data
            
        except Exception as e:
            logger.debug(f"Veri Ã§Ä±karma hatasÄ±: {e}")
            return None
    
    def extract_location_from_text(self, text: str) -> tuple:
        """Metinden il ve ilÃ§e bilgisini Ã§Ä±kar"""
        text = text.lower()
        
        # Ä°l adlarÄ±nÄ± ara
        for il_adi, il_kodu in self.il_kodlari.items():
            if il_adi in text:
                # Ä°lÃ§e iÃ§in basit tahmin
                ilce_patterns = [
                    r'(\w+)\s*/' + re.escape(il_adi),
                    re.escape(il_adi) + r'/(\w+)',
                    r'(\w+)\s*' + re.escape(il_adi),
                ]
                
                ilce_adi = "Merkez"
                for pattern in ilce_patterns:
                    match = re.search(pattern, text)
                    if match:
                        ilce_adi = match.group(1).title()
                        break
                
                return il_adi.title(), ilce_adi
        
        return "Bilinmiyor", "Bilinmiyor"
    
    def extract_university_name(self, kurum_adi: str) -> str:
        """Kurum adÄ±ndan Ã¼niversite adÄ±nÄ± Ã§Ä±kar"""
        # Ãœniversite adÄ± patterns
        patterns = [
            r'(.+?)\s+Ãœniversitesi',
            r'(.+?)\s+Ãœni\.',
            r'(.+?)\s+University',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, kurum_adi, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        return ""
    
    def is_university_hospital(self, hospital_data: Dict[str, Any]) -> bool:
        """Hastanenin Ã¼niversite hastanesi olup olmadÄ±ÄŸÄ±nÄ± kontrol et"""
        kurum_adi = hospital_data.get('kurum_adi', '').lower()
        
        university_keywords = [
            'Ã¼niversite', 'Ã¼niversitesi', 'university',
            'tÄ±p fakÃ¼ltesi', 'tÄ±p fak.', 'medical faculty',
            'eÄŸitim araÅŸtÄ±rma', 'eÄŸitim ve araÅŸtÄ±rma',
            'training and research'
        ]
        
        return any(keyword in kurum_adi for keyword in university_keywords)
    
    def scrape_all_universities(self) -> List[UniversiteHastanesi]:
        """TÃ¼m Ã¼niversite hastanelerini scrape et"""
        logger.info("TR Hastane Ã¼niversite hastaneleri scraping baÅŸlatÄ±lÄ±yor...")
        
        # URL'leri bul
        urls = self.find_university_hospitals_urls()
        
        if not urls:
            logger.warning("Ãœniversite hastanesi URL'leri bulunamadÄ±, demo veri Ã¼retiliyor...")
            return self.generate_demo_universities()
        
        all_hospitals = []
        
        for i, url in enumerate(urls, 1):
            logger.info(f"URL iÅŸleniyor ({i}/{len(urls)}): {url}")
            
            try:
                hospitals = self.extract_hospital_from_page(url)
                all_hospitals.extend(hospitals)
                
                logger.info(f"{len(hospitals)} Ã¼niversite hastanesi bulundu: {url}")
                
                # Rate limiting
                time.sleep(2)
                
            except Exception as e:
                logger.error(f"URL iÅŸleme hatasÄ± {url}: {e}")
        
        # Duplicate'larÄ± kaldÄ±r
        unique_hospitals = self.remove_duplicates(all_hospitals)
        
        # UniversiteHastanesi objelerine dÃ¶nÃ¼ÅŸtÃ¼r
        result = []
        for hospital_data in unique_hospitals:
            hospital = UniversiteHastanesi(**hospital_data)
            result.append(hospital)
        
        logger.info(f"Toplam {len(result)} benzersiz Ã¼niversite hastanesi bulundu")
        
        if len(result) < 10:  # Ã‡ok az veri varsa demo ekle
            logger.info("Yeterli veri bulunamadÄ±, demo veriler ekleniyor...")
            demo_hospitals = self.generate_demo_universities()
            result.extend(demo_hospitals)
        
        return result
    
    def generate_demo_universities(self) -> List[UniversiteHastanesi]:
        """Demo Ã¼niversite hastaneleri oluÅŸtur"""
        demo_data = [
            {
                'kurum_adi': 'Marmara Ãœniversitesi Pendik EÄŸitim ve AraÅŸtÄ±rma Hastanesi',
                'il_adi': 'Ä°stanbul', 'ilce_adi': 'Pendik',
                'universite_adi': 'Marmara Ãœniversitesi'
            },
            {
                'kurum_adi': 'Dokuz EylÃ¼l Ãœniversitesi Hastanesi',
                'il_adi': 'Ä°zmir', 'ilce_adi': 'BalÃ§ova',
                'universite_adi': 'Dokuz EylÃ¼l Ãœniversitesi'
            },
            {
                'kurum_adi': 'Gazi Ãœniversitesi TÄ±p FakÃ¼ltesi Hastanesi',
                'il_adi': 'Ankara', 'ilce_adi': 'Yenimahalle',
                'universite_adi': 'Gazi Ãœniversitesi'
            },
            {
                'kurum_adi': 'SelÃ§uk Ãœniversitesi TÄ±p FakÃ¼ltesi Hastanesi',
                'il_adi': 'Konya', 'ilce_adi': 'SelÃ§uklu',
                'universite_adi': 'SelÃ§uk Ãœniversitesi'
            },
            {
                'kurum_adi': 'Erciyes Ãœniversitesi TÄ±p FakÃ¼ltesi Hastanesi',
                'il_adi': 'Kayseri', 'ilce_adi': 'Melikgazi',
                'universite_adi': 'Erciyes Ãœniversitesi'
            }
        ]
        
        demo_hospitals = []
        for data in demo_data:
            hospital_data = {
                'kurum_adi': data['kurum_adi'],
                'kurum_tipi': 'Ãœniversite Hastanesi',
                'il_adi': data['il_adi'],
                'il_kodu': self.get_il_kodu(data['il_adi']),
                'ilce_adi': data['ilce_adi'],
                'adres': f"{data['il_adi']} {data['ilce_adi']}",
                'telefon': '',
                'koordinat_lat': None,
                'koordinat_lon': None,
                'web_sitesi': '',
                'veri_kaynagi': 'TR Hastane - Demo Ãœniversite Verileri',
                'son_guncelleme': datetime.now().strftime('%Y-%m-%d'),
                'trhastane_url': None,
                'universite_adi': data['universite_adi']
            }
            
            hospital_data['kurum_id'] = self.generate_kurum_id(hospital_data)
            hospital = UniversiteHastanesi(**hospital_data)
            demo_hospitals.append(hospital)
        
        return demo_hospitals
    
    def remove_duplicates(self, hospitals: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Duplicate hastaneleri kaldÄ±r"""
        seen = set()
        unique_hospitals = []
        
        for hospital in hospitals:
            # Ad + il kombinasyonu ile duplicate kontrolÃ¼
            key = f"{hospital['kurum_adi']}-{hospital['il_adi']}"
            normalized_key = self.normalize_text(key.lower())
            
            if normalized_key not in seen:
                seen.add(normalized_key)
                unique_hospitals.append(hospital)
        
        return unique_hospitals
    
    def save_to_file(self, hospitals: List[UniversiteHastanesi], filename: str):
        """Hastaneleri JSON dosyasÄ±na kaydet"""
        try:
            os.makedirs('data/raw', exist_ok=True)
            
            # Dataclass'larÄ± dict'e dÃ¶nÃ¼ÅŸtÃ¼r
            data = [asdict(hospital) for hospital in hospitals]
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… {len(hospitals)} Ã¼niversite hastanesi {filename} dosyasÄ±na kaydedildi")
            
        except Exception as e:
            logger.error(f"âŒ Dosya kaydetme hatasÄ±: {e}")

def main():
    """Ana fonksiyon"""
    try:
        logger.info("ğŸ¥ TR Hastane Ãœniversite Hastaneleri Scraper BaÅŸlatÄ±lÄ±yor...")
        
        scraper = TRHastaneUniversiteScraper()
        
        # Scraping iÅŸlemi
        hospitals = scraper.scrape_all_universities()
        
        if hospitals:
            # Dosyaya kaydet
            output_file = 'data/raw/trhastane_universite_hastaneleri.json'
            scraper.save_to_file(hospitals, output_file)
            
            # Ä°statistikler
            il_dagilimi = {}
            for hospital in hospitals:
                il = hospital.il_adi
                il_dagilimi[il] = il_dagilimi.get(il, 0) + 1
            
            logger.info("ğŸ“Š Ä°l DaÄŸÄ±lÄ±mÄ±:")
            for il, count in sorted(il_dagilimi.items()):
                logger.info(f"   {il}: {count} hastane")
            
            logger.info(f"ğŸ¯ Toplam {len(hospitals)} Ã¼niversite hastanesi baÅŸarÄ±yla scrape edildi!")
            
        else:
            logger.error("âŒ HiÃ§ Ã¼niversite hastanesi bulunamadÄ±!")
            return 1
        
        return 0
        
    except Exception as e:
        logger.error(f"âŒ Scraping hatasÄ±: {e}")
        return 1

if __name__ == "__main__":
    exit(main())
