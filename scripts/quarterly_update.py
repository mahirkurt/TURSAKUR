#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ÃœÃ§ AylÄ±k Otomatik GÃ¼ncelleme ModÃ¼lÃ¼

Bu modÃ¼l, 3 ayda bir Ã§alÄ±ÅŸarak tÃ¼m saÄŸlÄ±k kurumu verilerini gÃ¼nceller,
deÄŸiÅŸiklikleri analiz eder ve raporlar.

Author: TURSAKUR Team
Version: 1.0.0
Date: 2025-01-14
"""

import json
import logging
import os
import shutil
import subprocess
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Set, Tuple
import difflib
from dataclasses import dataclass, asdict
from pathlib import Path

# Logging konfigÃ¼rasyonu
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/quarterly_update.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class HealthInstitution:
    """SaÄŸlÄ±k kurumu veri yapÄ±sÄ±"""
    kurum_id: str
    kurum_adi: str
    kurum_tipi: str
    il_kodu: int
    il_adi: str
    ilce_adi: str
    adres: str
    telefon: str
    koordinat_lat: float
    koordinat_lon: float
    web_sitesi: str
    veri_kaynagi: str
    son_guncelleme: str

@dataclass
class ChangeAnalysis:
    """DeÄŸiÅŸiklik analizi veri yapÄ±sÄ±"""
    new_institutions: List[HealthInstitution]
    removed_institutions: List[HealthInstitution]
    modified_institutions: List[Tuple[HealthInstitution, HealthInstitution]]  # (old, new)
    unchanged_count: int

class QuarterlyUpdater:
    """ÃœÃ§ aylÄ±k gÃ¼ncelleme sÄ±nÄ±fÄ±"""
    
    def __init__(self):
        self.backup_dir = f"backups/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}"
        self.current_data_file = 'data/turkiye_saglik_kuruluslari.json'
        self.temp_data_file = 'data/turkiye_saglik_kuruluslari_temp.json'
        
    def should_run_update(self) -> bool:
        """GÃ¼ncellemenin Ã§alÄ±ÅŸtÄ±rÄ±lÄ±p Ã§alÄ±ÅŸtÄ±rÄ±lmayacaÄŸÄ±nÄ± kontrol et"""
        
        # Son gÃ¼ncelleme tarihini kontrol et
        metadata_file = 'data/update_metadata.json'
        
        if not os.path.exists(metadata_file):
            logger.info("Ä°lk gÃ¼ncelleme - Ã§alÄ±ÅŸtÄ±rÄ±lacak")
            return True
        
        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            last_update = datetime.fromisoformat(metadata.get('last_update', '2020-01-01'))
            next_update = last_update + timedelta(days=90)  # 3 ay
            
            if datetime.now() >= next_update:
                logger.info(f"Son gÃ¼ncelleme: {last_update.strftime('%Y-%m-%d')}, gÃ¼ncelleme zamanÄ± geldi")
                return True
            else:
                logger.info(f"HenÃ¼z gÃ¼ncelleme zamanÄ± deÄŸil. Sonraki gÃ¼ncelleme: {next_update.strftime('%Y-%m-%d')}")
                return False
                
        except Exception as e:
            logger.warning(f"Metadata kontrol hatasÄ±: {e} - GÃ¼ncelleme Ã§alÄ±ÅŸtÄ±rÄ±lacak")
            return True
    
    def create_backup(self) -> bool:
        """Mevcut verilerin yedeÄŸini oluÅŸtur"""
        try:
            logger.info(f"Backup oluÅŸturuluyor: {self.backup_dir}")
            
            # Backup dizinini oluÅŸtur
            os.makedirs(self.backup_dir, exist_ok=True)
            
            # Ana veri dosyasÄ±nÄ± yedekle
            if os.path.exists(self.current_data_file):
                backup_file = os.path.join(self.backup_dir, 'turkiye_saglik_kuruluslari.json')
                shutil.copy2(self.current_data_file, backup_file)
                logger.info(f"Ana veri dosyasÄ± yedeklendi: {backup_file}")
            
            # Raw data klasÃ¶rÃ¼nÃ¼ yedekle
            raw_data_dir = 'data/raw'
            if os.path.exists(raw_data_dir):
                backup_raw_dir = os.path.join(self.backup_dir, 'raw')
                shutil.copytree(raw_data_dir, backup_raw_dir)
                logger.info(f"Raw data klasÃ¶rÃ¼ yedeklendi: {backup_raw_dir}")
            
            # Log dosyalarÄ±nÄ± yedekle
            logs_dir = 'logs'
            if os.path.exists(logs_dir):
                backup_logs_dir = os.path.join(self.backup_dir, 'logs')
                shutil.copytree(logs_dir, backup_logs_dir)
                logger.info(f"Log dosyalarÄ± yedeklendi: {backup_logs_dir}")
            
            return True
            
        except Exception as e:
            logger.error(f"Backup oluÅŸturma hatasÄ±: {e}")
            return False
    
    def load_current_data(self) -> List[HealthInstitution]:
        """Mevcut veriyi yÃ¼kle"""
        try:
            if not os.path.exists(self.current_data_file):
                logger.info("Mevcut veri dosyasÄ± bulunamadÄ± - ilk Ã§alÄ±ÅŸtÄ±rma")
                return []
            
            with open(self.current_data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            institutions = []
            for item in data:
                institution = HealthInstitution(**item)
                institutions.append(institution)
            
            logger.info(f"Mevcut veri yÃ¼klendi: {len(institutions)} kurum")
            return institutions
            
        except Exception as e:
            logger.error(f"Mevcut veri yÃ¼kleme hatasÄ±: {e}")
            return []
    
    def run_data_collection(self) -> bool:
        """Yeni veri toplama iÅŸlemini Ã§alÄ±ÅŸtÄ±r"""
        try:
            logger.info("Yeni veri toplama iÅŸlemi baÅŸlatÄ±lÄ±yor...")
            
            collection_script = 'scripts/fetch_all_sources.py'
            
            if not os.path.exists(collection_script):
                logger.error(f"Veri toplama script'i bulunamadÄ±: {collection_script}")
                return False
            
            # Veri toplama script'ini Ã§alÄ±ÅŸtÄ±r
            result = subprocess.run(
                [sys.executable, collection_script],
                capture_output=True,
                text=True,
                encoding='utf-8',
                timeout=3600  # 1 saat timeout
            )
            
            if result.returncode == 0:
                logger.info("Veri toplama baÅŸarÄ±yla tamamlandÄ±")
                
                # Yeni veriyi temp dosyaya taÅŸÄ±
                if os.path.exists(self.current_data_file):
                    shutil.move(self.current_data_file, self.temp_data_file)
                    logger.info("Yeni veri geÃ§ici dosyaya taÅŸÄ±ndÄ±")
                
                return True
            else:
                logger.error(f"Veri toplama hatasÄ±: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            logger.error("Veri toplama timeout ile sonlandÄ±")
            return False
        except Exception as e:
            logger.error(f"Veri toplama exception: {e}")
            return False
    
    def load_new_data(self) -> List[HealthInstitution]:
        """Yeni toplanan veriyi yÃ¼kle"""
        try:
            if not os.path.exists(self.temp_data_file):
                logger.error("Yeni veri dosyasÄ± bulunamadÄ±")
                return []
            
            with open(self.temp_data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            institutions = []
            for item in data:
                institution = HealthInstitution(**item)
                institutions.append(institution)
            
            logger.info(f"Yeni veri yÃ¼klendi: {len(institutions)} kurum")
            return institutions
            
        except Exception as e:
            logger.error(f"Yeni veri yÃ¼kleme hatasÄ±: {e}")
            return []
    
    def analyze_changes(self, old_data: List[HealthInstitution], 
                       new_data: List[HealthInstitution]) -> ChangeAnalysis:
        """DeÄŸiÅŸiklikleri analiz et"""
        logger.info("DeÄŸiÅŸiklik analizi baÅŸlatÄ±lÄ±yor...")
        
        # ID bazÄ±nda mapping oluÅŸtur
        old_by_id = {inst.kurum_id: inst for inst in old_data}
        new_by_id = {inst.kurum_id: inst for inst in new_data}
        
        # Ä°sim bazÄ±nda mapping (ID deÄŸiÅŸebilir)
        old_by_name = {self._normalize_name(inst.kurum_adi): inst for inst in old_data}
        new_by_name = {self._normalize_name(inst.kurum_adi): inst for inst in new_data}
        
        new_institutions = []
        removed_institutions = []
        modified_institutions = []
        unchanged_count = 0
        
        # Yeni kurumlarÄ± bul
        for inst in new_data:
            if inst.kurum_id not in old_by_id:
                # Ä°sim bazÄ±nda da kontrol et
                norm_name = self._normalize_name(inst.kurum_adi)
                if norm_name not in old_by_name:
                    new_institutions.append(inst)
        
        # KaldÄ±rÄ±lan kurumlarÄ± bul
        for inst in old_data:
            if inst.kurum_id not in new_by_id:
                # Ä°sim bazÄ±nda da kontrol et
                norm_name = self._normalize_name(inst.kurum_adi)
                if norm_name not in new_by_name:
                    removed_institutions.append(inst)
        
        # DeÄŸiÅŸtirilen kurumlarÄ± bul
        for inst_id in old_by_id:
            if inst_id in new_by_id:
                old_inst = old_by_id[inst_id]
                new_inst = new_by_id[inst_id]
                
                if self._has_significant_changes(old_inst, new_inst):
                    modified_institutions.append((old_inst, new_inst))
                else:
                    unchanged_count += 1
        
        analysis = ChangeAnalysis(
            new_institutions=new_institutions,
            removed_institutions=removed_institutions,
            modified_institutions=modified_institutions,
            unchanged_count=unchanged_count
        )
        
        logger.info(f"DeÄŸiÅŸiklik analizi tamamlandÄ±:")
        logger.info(f"  Yeni: {len(new_institutions)}")
        logger.info(f"  KaldÄ±rÄ±lan: {len(removed_institutions)}")
        logger.info(f"  DeÄŸiÅŸtirilen: {len(modified_institutions)}")
        logger.info(f"  DeÄŸiÅŸmeyen: {unchanged_count}")
        
        return analysis
    
    def _normalize_name(self, name: str) -> str:
        """Kurum adÄ±nÄ± normalize et"""
        if not name:
            return ""
        
        # KÃ¼Ã§Ã¼k harfe Ã§evir ve fazla boÅŸluklarÄ± kaldÄ±r
        normalized = name.lower().strip()
        normalized = ' '.join(normalized.split())
        
        return normalized
    
    def _has_significant_changes(self, old_inst: HealthInstitution, 
                                new_inst: HealthInstitution) -> bool:
        """AnlamlÄ± deÄŸiÅŸiklik olup olmadÄ±ÄŸÄ±nÄ± kontrol et"""
        
        # Ã–nemli alanlarÄ± karÅŸÄ±laÅŸtÄ±r
        significant_fields = [
            'kurum_adi', 'kurum_tipi', 'adres', 'telefon', 
            'koordinat_lat', 'koordinat_lon', 'web_sitesi'
        ]
        
        for field in significant_fields:
            old_value = getattr(old_inst, field)
            new_value = getattr(new_inst, field)
            
            # None deÄŸerleri boÅŸ string olarak kabul et
            old_value = old_value if old_value is not None else ""
            new_value = new_value if new_value is not None else ""
            
            if str(old_value).strip() != str(new_value).strip():
                return True
        
        return False
    
    def generate_change_report(self, analysis: ChangeAnalysis) -> str:
        """DeÄŸiÅŸiklik raporu oluÅŸtur"""
        
        report = f"""
=== TURSAKUR ÃœÃ‡ AYLIK GÃœNCELLEME RAPORU ===
Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

ğŸ“Š DEÄÄ°ÅÄ°KLÄ°K Ã–ZETÄ°:
â• Yeni kurumlar: {len(analysis.new_institutions)}
â– KaldÄ±rÄ±lan kurumlar: {len(analysis.removed_institutions)}
ğŸ”„ GÃ¼ncellenen kurumlar: {len(analysis.modified_institutions)}
âœ… DeÄŸiÅŸmeyen kurumlar: {analysis.unchanged_count}

Toplam kurum sayÄ±sÄ±: {len(analysis.new_institutions) + len(analysis.removed_institutions) + 
                      len(analysis.modified_institutions) + analysis.unchanged_count}

"""
        
        # Yeni kurumlar
        if analysis.new_institutions:
            report += "ğŸ“ˆ YENÄ° KURUMLAR:\n"
            for inst in analysis.new_institutions[:10]:  # Ä°lk 10'u gÃ¶ster
                report += f"  âœ… {inst.kurum_adi} ({inst.il_adi}) - {inst.kurum_tipi}\n"
            
            if len(analysis.new_institutions) > 10:
                report += f"  ... ve {len(analysis.new_institutions) - 10} kurum daha\n"
            report += "\n"
        
        # KaldÄ±rÄ±lan kurumlar
        if analysis.removed_institutions:
            report += "ğŸ“‰ KALDIRILAN KURUMLAR:\n"
            for inst in analysis.removed_institutions[:10]:  # Ä°lk 10'u gÃ¶ster
                report += f"  âŒ {inst.kurum_adi} ({inst.il_adi}) - {inst.kurum_tipi}\n"
            
            if len(analysis.removed_institutions) > 10:
                report += f"  ... ve {len(analysis.removed_institutions) - 10} kurum daha\n"
            report += "\n"
        
        # GÃ¼ncellenen kurumlar
        if analysis.modified_institutions:
            report += "ğŸ”„ GÃœNCELLENENLÄ°K KURUMLAR:\n"
            for old_inst, new_inst in analysis.modified_institutions[:5]:  # Ä°lk 5'i gÃ¶ster
                report += f"  ğŸ“ {new_inst.kurum_adi} ({new_inst.il_adi})\n"
                
                # DeÄŸiÅŸiklikleri detaylandÄ±r
                changes = []
                if old_inst.kurum_adi != new_inst.kurum_adi:
                    changes.append(f"Ad: '{old_inst.kurum_adi}' â†’ '{new_inst.kurum_adi}'")
                if old_inst.adres != new_inst.adres:
                    changes.append("Adres deÄŸiÅŸti")
                if old_inst.telefon != new_inst.telefon:
                    changes.append(f"Telefon: '{old_inst.telefon}' â†’ '{new_inst.telefon}'")
                
                for change in changes[:3]:  # En fazla 3 deÄŸiÅŸiklik gÃ¶ster
                    report += f"     â€¢ {change}\n"
            
            if len(analysis.modified_institutions) > 5:
                report += f"  ... ve {len(analysis.modified_institutions) - 5} kurum daha\n"
            report += "\n"
        
        # Ä°statistikler
        total_changes = len(analysis.new_institutions) + len(analysis.removed_institutions) + len(analysis.modified_institutions)
        total_institutions = total_changes + analysis.unchanged_count
        
        if total_institutions > 0:
            change_rate = (total_changes / total_institutions) * 100
            report += f"ğŸ“Š Ä°STATÄ°STÄ°KLER:\n"
            report += f"DeÄŸiÅŸim oranÄ±: {change_rate:.1f}%\n"
            report += f"Veri kalitesi: {'YÃ¼ksek' if change_rate < 10 else 'Orta' if change_rate < 25 else 'DÃ¼ÅŸÃ¼k'}\n"
        
        report += f"\nâ±ï¸  GÃ¼ncelleme tamamlandÄ±: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        
        return report
    
    def finalize_update(self, analysis: ChangeAnalysis) -> bool:
        """GÃ¼ncellemeyi sonlandÄ±r"""
        try:
            # Yeni veriyi ana dosyaya taÅŸÄ±
            if os.path.exists(self.temp_data_file):
                shutil.move(self.temp_data_file, self.current_data_file)
                logger.info("Yeni veri ana dosyaya taÅŸÄ±ndÄ±")
            
            # Metadata gÃ¼ncelle
            metadata = {
                'last_update': datetime.now().isoformat(),
                'new_institutions': len(analysis.new_institutions),
                'removed_institutions': len(analysis.removed_institutions),
                'modified_institutions': len(analysis.modified_institutions),
                'unchanged_count': analysis.unchanged_count,
                'backup_location': self.backup_dir
            }
            
            with open('data/update_metadata.json', 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            logger.info("Metadata gÃ¼ncellendi")
            return True
            
        except Exception as e:
            logger.error(f"GÃ¼ncelleme sonlandÄ±rma hatasÄ±: {e}")
            return False
    
    def cleanup_old_backups(self, keep_count: int = 5):
        """Eski backup'larÄ± temizle"""
        try:
            backups_dir = Path('backups')
            if not backups_dir.exists():
                return
            
            # Backup klasÃ¶rlerini tarihe gÃ¶re sÄ±rala
            backup_dirs = [d for d in backups_dir.iterdir() if d.is_dir()]
            backup_dirs.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            
            # Fazla backup'larÄ± sil
            for backup_dir in backup_dirs[keep_count:]:
                shutil.rmtree(backup_dir)
                logger.info(f"Eski backup silindi: {backup_dir}")
                
        except Exception as e:
            logger.warning(f"Backup temizleme hatasÄ±: {e}")

def main():
    """Ana fonksiyon"""
    try:
        logger.info("TURSAKUR Ã¼Ã§ aylÄ±k gÃ¼ncelleme kontrolleri baÅŸlatÄ±lÄ±yor...")
        
        updater = QuarterlyUpdater()
        
        # GÃ¼ncelleme gerekli mi kontrol et
        if not updater.should_run_update():
            logger.info("GÃ¼ncelleme gerekli deÄŸil")
            return 0
        
        # 1. Backup oluÅŸtur
        print("ğŸ’¾ Mevcut veriler yedekleniyor...")
        if not updater.create_backup():
            logger.error("Backup oluÅŸturulamadÄ± - gÃ¼ncelleme iptal ediliyor")
            return 1
        
        # 2. Mevcut veriyi yÃ¼kle
        print("ğŸ“Š Mevcut veriler yÃ¼kleniyor...")
        old_data = updater.load_current_data()
        
        # 3. Yeni veri toplama
        print("ğŸ”„ Yeni veriler toplanÄ±yor...")
        if not updater.run_data_collection():
            logger.error("Veri toplama baÅŸarÄ±sÄ±z - gÃ¼ncelleme iptal ediliyor")
            return 1
        
        # 4. Yeni veriyi yÃ¼kle
        print("ğŸ“¥ Yeni veriler yÃ¼kleniyor...")
        new_data = updater.load_new_data()
        
        if not new_data:
            logger.error("Yeni veri yÃ¼klenemedi - gÃ¼ncelleme iptal ediliyor")
            return 1
        
        # 5. DeÄŸiÅŸiklikleri analiz et
        print("ğŸ” DeÄŸiÅŸiklikler analiz ediliyor...")
        analysis = updater.analyze_changes(old_data, new_data)
        
        # 6. Rapor oluÅŸtur
        report = updater.generate_change_report(analysis)
        print(report)
        
        # Raporu dosyaya kaydet
        report_file = f"logs/quarterly_report_{datetime.now().strftime('%Y-%m-%d')}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        # 7. GÃ¼ncellemeyi sonlandÄ±r
        print("âœ… GÃ¼ncelleme sonlandÄ±rÄ±lÄ±yor...")
        if not updater.finalize_update(analysis):
            logger.error("GÃ¼ncelleme sonlandÄ±rÄ±lamadÄ±")
            return 1
        
        # 8. Eski backup'larÄ± temizle
        updater.cleanup_old_backups()
        
        logger.info("ÃœÃ§ aylÄ±k gÃ¼ncelleme baÅŸarÄ±yla tamamlandÄ±!")
        return 0
        
    except Exception as e:
        logger.error(f"Ana fonksiyon kritik hatasÄ±: {e}")
        return 1

if __name__ == "__main__":
    exit(main())
