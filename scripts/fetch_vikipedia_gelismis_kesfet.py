#!/usr/bin/env python3
"""
Vikipedia SaÄŸlÄ±k KuruluÅŸlarÄ± GeliÅŸmiÅŸ Veri KaynaklarÄ± Scripti
=============================================================

Vikipedia'daki saÄŸlÄ±k kuruluÅŸlarÄ± verilerini geniÅŸletme:
1. TÃ¼rkiye'deki hastaneler kategorisi ve alt kategorileri
2. Ãœniversite hastaneleri listesi
3. Åehir bazlÄ± hastane listeleri
4. SaÄŸlÄ±k kuruluÅŸlarÄ± infobox verileri
5. Wikidata entegrasyonu ile koordinat ve detay bilgileri
"""

import requests
import json
import os
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
from bs4 import BeautifulSoup
import re
import urllib.parse

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class VikipediaGelismisKesfet:
    """Vikipedia'dan geliÅŸmiÅŸ saÄŸlÄ±k kuruluÅŸlarÄ± verilerini Ã§eken sÄ±nÄ±f."""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'TursakurBot/1.0 (https://github.com/turkiye-saglik-kuruluslari; info@turkiye-saglik.org)'
        })
        
        # Wikipedia API endpoints
        self.wiki_api = 'https://tr.wikipedia.org/api/rest_v1'
        self.wikidata_api = 'https://www.wikidata.org/w/api.php'
        
        # SaÄŸlÄ±k kuruluÅŸlarÄ± ile ilgili Vikipedia kategorileri
        self.wiki_categories = {
            'hastaneler': [
                'Kategori:TÃ¼rkiye\'deki hastaneler',
                'Kategori:Ä°stanbul\'daki hastaneler',
                'Kategori:Ankara\'daki hastaneler',
                'Kategori:Ä°zmir\'deki hastaneler',
                'Kategori:Bursa\'daki hastaneler',
                'Kategori:Antalya\'daki hastaneler',
                'Kategori:Adana\'daki hastaneler'
            ],
            'universite_hastaneleri': [
                'Kategori:TÃ¼rkiye\'deki Ã¼niversite hastaneleri',
                'Kategori:TÄ±p fakÃ¼lteleri'
            ],
            'saglik_kuruluslari': [
                'Kategori:TÃ¼rkiye\'deki saÄŸlÄ±k kuruluÅŸlarÄ±',
                'Kategori:SaÄŸlÄ±k bakanlÄ±ÄŸÄ± hastaneleri',
                'Kategori:Ã–zel hastaneler'
            ],
            'tip_fakulteleri': [
                'Kategori:TÃ¼rkiye\'deki tÄ±p fakÃ¼lteleri',
                'Kategori:TÄ±p eÄŸitimi'
            ]
        }
        
        # Åehir bazlÄ± saÄŸlÄ±k kuruluÅŸlarÄ± arama listesi
        self.major_cities = [
            'Ä°stanbul', 'Ankara', 'Ä°zmir', 'Bursa', 'Antalya', 'Adana', 'Konya',
            'Gaziantep', 'Kayseri', 'EskiÅŸehir', 'Mersin', 'Kocaeli', 'Trabzon',
            'Samsun', 'DiyarbakÄ±r', 'Malatya', 'Erzurum', 'Van', 'Denizli',
            'Sakarya', 'TekirdaÄŸ', 'BalÄ±kesir', 'KÃ¼tahya', 'Manisa', 'Hatay'
        ]
        
        self.data_dir = os.path.join(os.path.dirname(__file__), '..', 'data', 'raw')
        os.makedirs(self.data_dir, exist_ok=True)
    
    def fetch_category_members(self, category: str) -> List[str]:
        """Vikipedia kategorisindeki Ã¼yeleri Ã§ek"""
        members = []
        
        try:
            # MediaWiki API kullanarak kategori Ã¼yelerini al
            params = {
                'action': 'query',
                'list': 'categorymembers',
                'cmtitle': category,
                'cmlimit': 500,
                'format': 'json'
            }
            
            response = self.session.get('https://tr.wikipedia.org/w/api.php', params=params)
            response.raise_for_status()
            
            data = response.json()
            
            if 'query' in data and 'categorymembers' in data['query']:
                for member in data['query']['categorymembers']:
                    title = member['title']
                    # Sadece ana ad alanÄ±ndaki sayfalarÄ± al (Kategori: vs. Åablon: hariÃ§)
                    if ':' not in title or title.startswith('Kategori:'):
                        continue
                    members.append(title)
                    
            logger.info(f"âœ“ {category}: {len(members)} Ã¼ye bulundu")
            
        except Exception as e:
            logger.warning(f"Kategori Ã¼yeleri alÄ±namadÄ± {category}: {e}")
        
        return members
    
    def fetch_page_info(self, page_title: str) -> Optional[Dict]:
        """Vikipedia sayfasÄ±ndan detay bilgileri Ã§ek"""
        try:
            # Sayfa iÃ§eriÄŸini al
            encoded_title = urllib.parse.quote(page_title.replace(' ', '_'))
            url = f"https://tr.wikipedia.org/wiki/{encoded_title}"
            
            response = self.session.get(url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Infobox verilerini Ã§Ä±kar
            infobox = soup.find('table', class_='infobox')
            info_data = {}
            
            if infobox:
                rows = infobox.find_all('tr')
                for row in rows:
                    cells = row.find_all(['th', 'td'])
                    if len(cells) >= 2:
                        key = cells[0].get_text(strip=True)
                        value = cells[1].get_text(strip=True)
                        info_data[key] = value
            
            # Koordinat bilgilerini ara
            coordinates = self._extract_coordinates(soup)
            
            # Sayfa kategorilerini al
            categories = []
            cat_links = soup.find_all('a', href=re.compile(r'/wiki/Kategori:'))
            for link in cat_links:
                cat_text = link.get_text(strip=True)
                if any(keyword in cat_text.lower() for keyword in ['hastane', 'saÄŸlÄ±k', 'tÄ±p', 'hospital']):
                    categories.append(cat_text)
            
            institution_data = {
                'kurum_adi': page_title,
                'wiki_url': url,
                'infobox_data': info_data,
                'koordinatlar': coordinates,
                'kategoriler': categories,
                'veri_kaynagi': 'Vikipedia GeliÅŸmiÅŸ KeÅŸif',
                'son_guncelleme': datetime.now().strftime('%Y-%m-%d')
            }
            
            # Infobox'tan ek bilgileri Ã§Ä±kar
            institution_data.update(self._parse_infobox_details(info_data))
            
            return institution_data
            
        except Exception as e:
            logger.warning(f"Sayfa bilgileri alÄ±namadÄ± {page_title}: {e}")
            return None
    
    def _extract_coordinates(self, soup: BeautifulSoup) -> Optional[Dict]:
        """Sayfadan koordinat bilgilerini Ã§Ä±kar"""
        # Geo microformat'Ä± ara
        geo_span = soup.find('span', class_='geo')
        if geo_span:
            coord_text = geo_span.get_text(strip=True)
            coords = coord_text.split(';')
            if len(coords) == 2:
                try:
                    lat = float(coords[0].strip())
                    lon = float(coords[1].strip())
                    return {'lat': lat, 'lon': lon}
                except ValueError:
                    pass
        
        # Koordinat linklerini ara
        coord_links = soup.find_all('a', href=re.compile(r'geohack|maps\.google'))
        for link in coord_links:
            href = link.get('href')
            # URL'den koordinatlarÄ± Ã§Ä±karmaya Ã§alÄ±ÅŸ
            lat_match = re.search(r'lat[=:]([0-9.-]+)', href)
            lon_match = re.search(r'lon[=:]([0-9.-]+)', href)
            
            if lat_match and lon_match:
                try:
                    lat = float(lat_match.group(1))
                    lon = float(lon_match.group(1))
                    return {'lat': lat, 'lon': lon}
                except ValueError:
                    pass
        
        return None
    
    def _parse_infobox_details(self, infobox_data: Dict) -> Dict:
        """Infobox verilerinden standart alanlarÄ± Ã§Ä±kar"""
        details = {}
        
        # YaygÄ±n infobox alan isimleri
        field_mappings = {
            'kuruluÅŸ': 'kurulus_tarihi',
            'kuruluÅŸ tarihi': 'kurulus_tarihi',
            'established': 'kurulus_tarihi',
            'aÃ§Ä±lÄ±ÅŸ': 'kurulus_tarihi',
            'konum': 'adres',
            'adres': 'adres',
            'location': 'adres',
            'address': 'adres',
            'yatak sayÄ±sÄ±': 'yatak_sayisi',
            'beds': 'yatak_sayisi',
            'capacity': 'yatak_sayisi',
            'telefon': 'telefon',
            'phone': 'telefon',
            'web sitesi': 'web_sitesi',
            'website': 'web_sitesi',
            'internet sitesi': 'web_sitesi',
            'tip': 'kurum_tipi',
            'type': 'kurum_tipi',
            'tÃ¼rÃ¼': 'kurum_tipi',
            'speciality': 'uzmanlik_alani',
            'uzmanlÄ±k': 'uzmanlik_alani',
            'dal': 'uzmanlik_alani'
        }
        
        for original_key, value in infobox_data.items():
            key_lower = original_key.lower().strip()
            
            # EÅŸleÅŸen alanÄ± bul
            for search_key, standard_field in field_mappings.items():
                if search_key in key_lower:
                    details[standard_field] = value.strip()
                    break
        
        # Kurum tÃ¼rÃ¼nÃ¼ belirle
        if 'kurum_tipi' not in details:
            details['kurum_tipi'] = self._determine_institution_type_from_infobox(infobox_data)
        
        return details
    
    def _determine_institution_type_from_infobox(self, infobox_data: Dict) -> str:
        """Infobox verilerinden kurum tÃ¼rÃ¼nÃ¼ belirle"""
        combined_text = ' '.join(infobox_data.values()).lower()
        
        if 'Ã¼niversite' in combined_text:
            return 'Ãœniversite Hastanesi'
        elif 'Ã¶zel' in combined_text:
            return 'Ã–zel Hastane'
        elif 'devlet' in combined_text or 'kamu' in combined_text:
            return 'Devlet Hastanesi'
        elif 'eÄŸitim' in combined_text or 'araÅŸtÄ±rma' in combined_text:
            return 'EÄŸitim ve AraÅŸtÄ±rma Hastanesi'
        elif 'Ã§ocuk' in combined_text:
            return 'Ã‡ocuk Hastanesi'
        elif 'kadÄ±n' in combined_text or 'doÄŸum' in combined_text:
            return 'KadÄ±n DoÄŸum Hastanesi'
        elif 'kalp' in combined_text or 'kardiyoloji' in combined_text:
            return 'Kardiyoloji Hastanesi'
        elif 'onkoloji' in combined_text or 'kanser' in combined_text:
            return 'Onkoloji Hastanesi'
        else:
            return 'Hastane'
    
    def search_city_hospitals(self, city_name: str) -> List[str]:
        """Åehir bazlÄ± hastane arama"""
        hospitals = []
        
        try:
            # Åehir + hastane kombinasyonu ile arama
            search_terms = [
                f"{city_name} hastaneleri",
                f"{city_name} saÄŸlÄ±k kuruluÅŸlarÄ±",
                f"{city_name} tÄ±p merkezi",
                f"Liste:{city_name} hastaneleri"
            ]
            
            for term in search_terms:
                params = {
                    'action': 'query',
                    'list': 'search',
                    'srsearch': term,
                    'srlimit': 50,
                    'format': 'json'
                }
                
                response = self.session.get('https://tr.wikipedia.org/w/api.php', params=params)
                response.raise_for_status()
                
                data = response.json()
                
                if 'query' in data and 'search' in data['query']:
                    for result in data['query']['search']:
                        title = result['title']
                        snippet = result.get('snippet', '')
                        
                        # SaÄŸlÄ±k kuruluÅŸu olup olmadÄ±ÄŸÄ±nÄ± kontrol et
                        if self._is_health_related(title, snippet):
                            hospitals.append(title)
            
            # DuplikatlarÄ± temizle
            hospitals = list(set(hospitals))
            logger.info(f"âœ“ {city_name}: {len(hospitals)} hastane bulundu")
            
        except Exception as e:
            logger.warning(f"Åehir aramasÄ± baÅŸarÄ±sÄ±z {city_name}: {e}")
        
        return hospitals
    
    def _is_health_related(self, title: str, snippet: str) -> bool:
        """BaÅŸlÄ±k ve snippet'in saÄŸlÄ±k kurumuyla ilgili olup olmadÄ±ÄŸÄ±nÄ± kontrol et"""
        combined_text = (title + ' ' + snippet).lower()
        
        health_keywords = [
            'hastane', 'hospital', 'saÄŸlÄ±k', 'health', 'tÄ±p', 'medical',
            'poliklinik', 'clinic', 'merkez', 'center', 'Ã¼niversitesi',
            'fakÃ¼ltesi', 'araÅŸtÄ±rma', 'eÄŸitim', 'Ã¶zel', 'devlet'
        ]
        
        return any(keyword in combined_text for keyword in health_keywords)
    
    def fetch_wikidata_details(self, page_title: str) -> Optional[Dict]:
        """Wikidata'dan ek detaylarÄ± Ã§ek"""
        try:
            # Ã–nce Vikipedia'dan Wikidata ID'sini al
            params = {
                'action': 'query',
                'titles': page_title,
                'prop': 'pageprops',
                'format': 'json'
            }
            
            response = self.session.get('https://tr.wikipedia.org/w/api.php', params=params)
            data = response.json()
            
            wikidata_id = None
            if 'query' in data and 'pages' in data['query']:
                for page_id, page_data in data['query']['pages'].items():
                    if 'pageprops' in page_data and 'wikibase_item' in page_data['pageprops']:
                        wikidata_id = page_data['pageprops']['wikibase_item']
                        break
            
            if not wikidata_id:
                return None
            
            # Wikidata'dan detaylarÄ± al
            wikidata_params = {
                'action': 'wbgetentities',
                'ids': wikidata_id,
                'format': 'json',
                'languages': 'tr|en'
            }
            
            wikidata_response = self.session.get(self.wikidata_api, params=wikidata_params)
            wikidata_data = wikidata_response.json()
            
            if 'entities' in wikidata_data and wikidata_id in wikidata_data['entities']:
                entity = wikidata_data['entities'][wikidata_id]
                
                details = {
                    'wikidata_id': wikidata_id
                }
                
                # Claims'den yararlÄ± bilgileri Ã§Ä±kar
                if 'claims' in entity:
                    claims = entity['claims']
                    
                    # Koordinatlar (P625)
                    if 'P625' in claims:
                        coord_claim = claims['P625'][0]
                        if 'mainsnak' in coord_claim and 'datavalue' in coord_claim['mainsnak']:
                            coords = coord_claim['mainsnak']['datavalue']['value']
                            details['wikidata_koordinatlar'] = {
                                'lat': coords['latitude'],
                                'lon': coords['longitude']
                            }
                    
                    # KuruluÅŸ tarihi (P571)
                    if 'P571' in claims:
                        founding_claim = claims['P571'][0]
                        if 'mainsnak' in founding_claim and 'datavalue' in founding_claim['mainsnak']:
                            details['wikidata_kurulus_tarihi'] = founding_claim['mainsnak']['datavalue']['value']['time']
                
                return details
                
        except Exception as e:
            logger.warning(f"Wikidata detaylarÄ± alÄ±namadÄ± {page_title}: {e}")
        
        return None
    
    def fetch_all_wikipedia_sources(self) -> List[Dict]:
        """TÃ¼m Vikipedia kaynaklarÄ±ndan saÄŸlÄ±k kuruluÅŸlarÄ±nÄ± Ã§ek"""
        all_institutions = []
        
        logger.info("ğŸ” Vikipedia geliÅŸmiÅŸ keÅŸif baÅŸlÄ±yor...")
        
        # 1. Kategori bazlÄ± tarama
        all_pages = set()
        for category_group, categories in self.wiki_categories.items():
            logger.info(f"ğŸ“‚ {category_group} kategorileri taranÄ±yor...")
            
            for category in categories:
                members = self.fetch_category_members(category)
                all_pages.update(members)
        
        logger.info(f"âœ… Kategori taramasÄ±: {len(all_pages)} benzersiz sayfa")
        
        # 2. Åehir bazlÄ± arama
        for city in self.major_cities:
            city_hospitals = self.search_city_hospitals(city)
            all_pages.update(city_hospitals)
        
        logger.info(f"âœ… Åehir aramasÄ± sonrasÄ±: {len(all_pages)} benzersiz sayfa")
        
        # 3. Her sayfa iÃ§in detay bilgileri Ã§ek
        for i, page_title in enumerate(all_pages, 1):
            if i % 10 == 0:
                logger.info(f"Ä°ÅŸleniyor: {i}/{len(all_pages)}")
            
            page_info = self.fetch_page_info(page_title)
            if page_info:
                # Wikidata detaylarÄ±nÄ± da ekle
                wikidata_info = self.fetch_wikidata_details(page_title)
                if wikidata_info:
                    page_info.update(wikidata_info)
                
                all_institutions.append(page_info)
        
        logger.info(f"ğŸ¯ Toplam iÅŸlenen Vikipedia kurumu: {len(all_institutions)}")
        return all_institutions
    
    def save_data(self, institutions: List[Dict]):
        """Verileri kaydet"""
        json_file = os.path.join(self.data_dir, 'vikipedia_gelismis_kesfet.json')
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(institutions, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ Vikipedia geliÅŸmiÅŸ verileri kaydedildi: {json_file}")

def main():
    """Ana fonksiyon"""
    logger.info("Vikipedia saÄŸlÄ±k kuruluÅŸlarÄ± geliÅŸmiÅŸ keÅŸfi baÅŸlÄ±yor...")
    
    fetcher = VikipediaGelismisKesfet()
    
    try:
        institutions = fetcher.fetch_all_wikipedia_sources()
        
        if institutions:
            fetcher.save_data(institutions)
            
            # Ä°statistikler
            types = {}
            cities = {}
            with_coords = 0
            with_wikidata = 0
            
            for institution in institutions:
                inst_type = institution.get('kurum_tipi', 'Bilinmiyor')
                types[inst_type] = types.get(inst_type, 0) + 1
                
                # Åehir bilgisi Ã§Ä±kar
                for city in fetcher.major_cities:
                    if city.lower() in institution['kurum_adi'].lower():
                        cities[city] = cities.get(city, 0) + 1
                        break
                
                if institution.get('koordinatlar') or institution.get('wikidata_koordinatlar'):
                    with_coords += 1
                
                if institution.get('wikidata_id'):
                    with_wikidata += 1
            
            logger.info("ğŸ¥ Kurum tÃ¼rÃ¼ bazlÄ± daÄŸÄ±lÄ±m:")
            for inst_type, count in sorted(types.items(), key=lambda x: x[1], reverse=True):
                logger.info(f"   - {inst_type}: {count}")
            
            logger.info("ğŸ™ï¸ Åehir bazlÄ± daÄŸÄ±lÄ±m (top 10):")
            for city, count in sorted(cities.items(), key=lambda x: x[1], reverse=True)[:10]:
                logger.info(f"   - {city}: {count}")
            
            logger.info(f"ğŸ“ Koordinat bilgisi olan: {with_coords}")
            logger.info(f"ğŸ”— Wikidata entegrasyonu olan: {with_wikidata}")
                
            logger.info("âœ… Vikipedia geliÅŸmiÅŸ keÅŸif iÅŸlemi tamamlandÄ±!")
        else:
            logger.warning("âŒ HiÃ§ Vikipedia kurumu bulunamadÄ±!")
            
    except Exception as e:
        logger.error(f"âŒ Ä°ÅŸlem baÅŸarÄ±sÄ±z: {e}")

if __name__ == "__main__":
    main()
