#!/usr/bin/env python3
"""
TURSAKUR 2.0 - Simplified Data Processor
=================================

Toplanan ham verileri i≈üler, temizler ve birle≈ütirir.
"""

import json
import logging
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List
import sys

class SimpleDataProcessor:
    """TURSAKUR ham verilerini i≈üler ve temizler"""
    
    def __init__(self):
        self.scripts_dir = Path(__file__).parent
        self.data_dir = self.scripts_dir.parent / "data"
        self.raw_data_dir = self.data_dir / "raw"
        self.processed_data_dir = self.data_dir / "processed"
        
        # Ensure directories exist
        self.processed_data_dir.mkdir(parents=True, exist_ok=True)
        
        # Logging setup
        log_file = self.data_dir / "processing.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)

    def load_raw_data(self) -> Dict[str, List[Dict]]:
        """Ham veri dosyalarƒ±nƒ± y√ºkler"""
        self.logger.info("Ham veriler y√ºkleniyor...")
        
        raw_data = {}
        
        # Raw data directory'deki t√ºm JSON dosyalarƒ±nƒ± bul
        json_files = list(self.raw_data_dir.glob("*.json"))
        
        for file_path in json_files:
            source_name = file_path.stem
            self.logger.info(f"Y√ºkleniyor: {source_name}")
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                    # Veri yapƒ±sƒ±na g√∂re kayƒ±tlarƒ± √ßƒ±kar
                    if isinstance(data, dict):
                        if 'veriler' in data:  # Saƒülƒ±k Bakanlƒ±ƒüƒ± formatƒ±
                            all_records = []
                            for key, records in data['veriler'].items():
                                for record in records:
                                    record['kategori'] = key
                                    all_records.append(record)
                            records = all_records
                        else:
                            records = data.get('kayitlar', data.get('data', data.get('facilities', [])))
                    elif isinstance(data, list):
                        records = data
                    else:
                        self.logger.warning(f"Bilinmeyen veri formatƒ±: {source_name}")
                        records = []
                    
                    raw_data[source_name] = records
                    self.logger.info(f"‚úÖ {source_name}: {len(records)} kayƒ±t y√ºklendi")
                    
            except Exception as e:
                self.logger.error(f"‚ùå {source_name} y√ºklenemedi: {e}")
                raw_data[source_name] = []
        
        total_records = sum(len(records) for records in raw_data.values())
        self.logger.info(f"Toplam ham veri: {total_records:,} kayƒ±t, {len(raw_data)} kaynak")
        
        return raw_data

    def normalize_record(self, raw_record: Dict, source: str) -> Dict:
        """Ham kaydƒ± standart formata d√∂n√º≈üt√ºr√ºr"""
        
        # Temel alanlarƒ± √ßƒ±kar
        name = (
            raw_record.get('ad') or 
            raw_record.get('name') or 
            raw_record.get('kurum_adi') or 
            raw_record.get('facility_name') or 
            ""
        )
        
        facility_type = (
            raw_record.get('tur') or 
            raw_record.get('type') or 
            raw_record.get('kurum_turu') or 
            raw_record.get('kategori') or
            ""
        )
        
        province = (
            raw_record.get('il') or 
            raw_record.get('province') or 
            raw_record.get('sehir') or
            ""
        )
        
        district = (
            raw_record.get('ilce') or 
            raw_record.get('district') or 
            ""
        )
        
        address = (
            raw_record.get('adres') or 
            raw_record.get('address') or 
            ""
        )
        
        phone = (
            raw_record.get('telefon') or 
            raw_record.get('phone') or 
            ""
        )
        
        website = (
            raw_record.get('website') or 
            raw_record.get('web_site') or 
            ""
        )
        
        # Koordinatlar
        latitude = raw_record.get('enlem') or raw_record.get('latitude')
        longitude = raw_record.get('boylam') or raw_record.get('longitude')
        
        try:
            latitude = float(latitude) if latitude is not None else None
            longitude = float(longitude) if longitude is not None else None
        except (ValueError, TypeError):
            latitude = longitude = None
        
        # Standart kayƒ±t olu≈ütur
        normalized = {
            'name': name.strip() if name else "",
            'facility_type': facility_type.strip() if facility_type else "Saƒülƒ±k Kurulu≈üu",
            'province': province.strip() if province else "",
            'district': district.strip() if district else "",
            'address': address.strip() if address else "",
            'phone': phone.strip() if phone else None,
            'website': website.strip() if website else None,
            'latitude': latitude,
            'longitude': longitude,
            'sources': [source],
            'created_at': datetime.now(timezone.utc).isoformat(),
            'updated_at': datetime.now(timezone.utc).isoformat()
        }
        
        return normalized

    def save_processed_data(self, records: List[Dict]):
        """ƒ∞≈ülenmi≈ü veriyi kaydeder"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # Supabase formatƒ±nda kaydet
        supabase_file = self.processed_data_dir / f"supabase_ready_{timestamp}.json"
        
        with open(supabase_file, 'w', encoding='utf-8') as f:
            json.dump(records, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ƒ∞≈ülenmi≈ü veri kaydedildi: {supabase_file}")
        self.logger.info(f"Toplam kayƒ±t: {len(records)}")

    def process(self) -> bool:
        """Ana i≈üleme fonksiyonu"""
        try:
            self.logger.info("TURSAKUR 2.0 veri i≈üleme ba≈ülƒ±yor...")
            
            # 1. Ham verileri y√ºkle
            raw_data = self.load_raw_data()
            if not raw_data:
                self.logger.error("Ham veri bulunamadƒ±!")
                return False
            
            # 2. T√ºm kayƒ±tlarƒ± normalize et
            processed_records = []
            for source, records in raw_data.items():
                for record in records:
                    normalized = self.normalize_record(record, source)
                    if normalized['name']:  # ƒ∞smi olan kayƒ±tlarƒ± al
                        processed_records.append(normalized)
            
            self.logger.info(f"ƒ∞≈ülenen kayƒ±t sayƒ±sƒ±: {len(processed_records)}")
            
            # 3. Veriyi kaydet
            self.save_processed_data(processed_records)
            
            self.logger.info("‚úÖ Veri i≈üleme ba≈üarƒ±yla tamamlandƒ±!")
            return True
            
        except Exception as e:
            self.logger.error(f"üí• Veri i≈üleme hatasƒ±: {e}")
            return False

def main():
    """Ana fonksiyon"""
    processor = SimpleDataProcessor()
    success = processor.process()
    return 0 if success else 1

if __name__ == "__main__":
    exit(main())

def clean_phone_number(phone: str) -> str:
    """Telefon numarasƒ±nƒ± standart formata d√∂n√º≈üt√ºr√ºr."""
    if not phone:
        return ""
    
    # T√ºm bo≈üluklarƒ± ve √∂zel karakterleri temizle
    cleaned = ''.join(c for c in phone if c.isdigit() or c == '+')
    
    # T√ºrkiye formatƒ±na d√∂n√º≈üt√ºr
    if cleaned.startswith('0'):
        cleaned = '+90' + cleaned[1:]
    elif cleaned.startswith('90') and not cleaned.startswith('+90'):
        cleaned = '+' + cleaned
    elif not cleaned.startswith('+90') and len(cleaned) == 10:
        cleaned = '+90' + cleaned
    
    return cleaned

def apply_geographic_mapping(kurum: Dict[str, Any]) -> Dict[str, Any]:
    """Kuruma coƒürafi e≈üleme uygula"""
    # Basit coƒürafi e≈üleme
    kurum = geo_mapper.enhance_institution_coordinates(kurum)
    
    # Il kodu kontrol√º
    if not kurum.get('il_kodu') or kurum['il_kodu'] == 0:
        kurum['il_kodu'] = 6  # Default Ankara
    
    if not kurum.get('il_adi'):
        kurum['il_adi'] = "Bilinmiyor"
    
    if not kurum.get('ilce_adi'):
        kurum['ilce_adi'] = "Merkez"
    
    return kurum

def clean_text(text: str) -> str:
    """Metni temizler ve normalize eder."""
    if not text:
        return ""
    
    # Unicode normalize
    text = unicodedata.normalize('NFKD', str(text))
    
    # Fazla bo≈üluklarƒ± temizle
    text = ' '.join(text.split())
    
    # ƒ∞stenmeyen karakterleri temizle
    text = text.replace('\x00', '').replace('\r', '').replace('\n', ' ')
    
    return text.strip()

def clean_address(address: str, il_adi: Optional[str] = None, ilce_adi: Optional[str] = None) -> str:
    """Adresi temizler ve normalize eder."""
    if not address:
        return ""
    
    # Temel temizlik
    address = clean_text(address)
    
    # ƒ∞l ve il√ße bilgilerini kaldƒ±r (√ß√ºnk√º ayrƒ± alanlarda tutuyoruz)
    if il_adi:
        address = address.replace(il_adi, "").replace(il_adi.upper(), "")
    if ilce_adi:
        address = address.replace(ilce_adi, "").replace(ilce_adi.upper(), "")
    
    # Fazla bo≈üluklarƒ± temizle
    address = ' '.join(address.split())
    
    return address.strip()

def generate_kurum_id(kurum_data: Dict[str, Any]) -> str:
    """Kurum ID'si olu≈üturur: TR-[il_kodu]-[tip_kodu]-[sira_no]"""
    il_kodu = kurum_data.get('il_kodu', 0)
    kurum_tipi = kurum_data.get('kurum_tipi', 'GENEL')
    
    # Tip kodlarƒ±
    tip_kodlari = {
        'DEVLET_HASTANESI': 'DH',
        'UNIVERSITE_HASTANESI': 'UH', 
        'OZEL_HASTANE': 'OH',
        'SAGLIK_MERKEZI': 'SM',
        'SAGLIK_OCAGI': 'SO',
        'ECZANE': 'EC',
        'OZEL_TIP_MERKEZI': 'OTM',
        'AGIZ_DIS_SAGLIGI_MERKEZI': 'ADM',
        'EGITIM_ARASTIRMA_HASTANESI': 'EAH',
        'GENEL': 'GN'
    }
    
    tip_kodu = tip_kodlari.get(kurum_tipi, 'GN')
    
    # Basit sƒ±ra numarasƒ± (hash tabanlƒ±)
    kurum_adi = kurum_data.get('kurum_adi', '')
    adres = kurum_data.get('adres', '')
    sira_no = abs(hash(kurum_adi + adres)) % 9999 + 1
    
    return f"TR-{il_kodu:02d}-{tip_kodu}-{sira_no:04d}"

def normalize_kurum_tipi(tip: str) -> str:
    """Kurum tipini standartla≈ütƒ±rƒ±r."""
    if not tip:
        return 'GENEL'
    
    tip_cleaned = tip.strip()
    
    # Direkt e≈ülemeler (b√ºy√ºk harf problemi i√ßin)
    tip_esleme = {
        'Devlet Hastanesi': 'DEVLET_HASTANESI',
        'DEVLET HASTANESƒ∞': 'DEVLET_HASTANESI',
        'DEVLET HASTANESI': 'DEVLET_HASTANESI',
        'KAMU HASTANESƒ∞': 'DEVLET_HASTANESI',
        'KAMU HASTANESI': 'DEVLET_HASTANESI',
        '√úniversite Hastanesi': 'UNIVERSITE_HASTANESI',
        '√úNƒ∞VERSƒ∞TE HASTANESƒ∞': 'UNIVERSITE_HASTANESI',
        'UNIVERSITE HASTANESI': 'UNIVERSITE_HASTANESI',
        '√ñzel Hastane': 'OZEL_HASTANE',
        '√ñZEL HASTANE': 'OZEL_HASTANE',
        'OZEL HASTANE': 'OZEL_HASTANE',
        '√ñZEL HASTANESƒ∞': 'OZEL_HASTANE',
        'Saƒülƒ±k Merkezi': 'SAGLIK_MERKEZI',
        'SAƒûLIK MERKEZƒ∞': 'SAGLIK_MERKEZI',
        'SAGLIK MERKEZI': 'SAGLIK_MERKEZI',
        'Saƒülƒ±k Ocaƒüƒ±': 'SAGLIK_OCAGI',
        'SAƒûLIK OCAƒûI': 'SAGLIK_OCAGI',
        'SAGLIK OCAGI': 'SAGLIK_OCAGI',
        'Eczane': 'ECZANE',
        'ECZANE': 'ECZANE',
        '√ñzel Tƒ±p Merkezi': 'OZEL_TIP_MERKEZI',
        '√ñZEL TIP MERKEZƒ∞': 'OZEL_TIP_MERKEZI',
        'OZEL TIP MERKEZI': 'OZEL_TIP_MERKEZI',
        # Yeni kategoriler - hem orijinal hem b√ºy√ºk harf versiyonlarƒ±
        'Aƒüƒ±z ve Di≈ü Saƒülƒ±ƒüƒ± Merkezi': 'AGIZ_DIS_SAGLIGI_MERKEZI',
        'AƒûIZ VE Dƒ∞≈û SAƒûLIƒûI MERKEZƒ∞': 'AGIZ_DIS_SAGLIGI_MERKEZI',
        'AGIZ VE DIS SAGLIGI MERKEZI': 'AGIZ_DIS_SAGLIGI_MERKEZI',
        'Eƒüitim ve Ara≈ütƒ±rma Hastanesi': 'EGITIM_ARASTIRMA_HASTANESI',
        'Eƒûƒ∞Tƒ∞M VE ARA≈ûTIRMA HASTANESƒ∞': 'EGITIM_ARASTIRMA_HASTANESI',
        'EGITIM VE ARASTIRMA HASTANESI': 'EGITIM_ARASTIRMA_HASTANESI'
    }
    
    # √ñnce direkt e≈üleme dene
    if tip_cleaned in tip_esleme:
        return tip_esleme[tip_cleaned]
    
    # B√ºy√ºk harf versiyonunu dene
    tip_upper = tip_cleaned.upper()
    if tip_upper in tip_esleme:
        return tip_esleme[tip_upper]
    
    return 'GENEL'

def validate_coordinates(lat: float, lon: float) -> bool:
    """Koordinatlarƒ±n T√ºrkiye sƒ±nƒ±rlarƒ± i√ßinde olup olmadƒ±ƒüƒ±nƒ± kontrol eder."""
    # T√ºrkiye'nin yakla≈üƒ±k sƒ±nƒ±rlarƒ±
    return (35.8 <= lat <= 42.1) and (25.7 <= lon <= 44.8)

def process_saglik_bakanligi_data() -> List[Dict[str, Any]]:
    """Saƒülƒ±k Bakanlƒ±ƒüƒ± verilerini i≈üler."""
    kurumlar = []
    
    try:
        file_path = os.path.join('data', 'raw', 'saglik_bakanligi_tesisleri.json')
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        logger.info(f"Saƒülƒ±k Bakanlƒ±ƒüƒ±: {len(data)} kayƒ±t y√ºklendi")
        
        for item in data:
            kurum = {
                'kurum_adi': clean_text(item.get('kurum_adi', '')),
                'kurum_tipi': normalize_kurum_tipi(item.get('kurum_tipi', 'DEVLET_HASTANESI')),
                'il_adi': clean_text(item.get('il_adi', '')),
                'ilce_adi': clean_text(item.get('ilce_adi', 'Merkez')),
                'adres': clean_address(item.get('adres', '')),
                'telefon': clean_phone_number(item.get('telefon', '')),
                'veri_kaynagi': 'T.C. Saƒülƒ±k Bakanlƒ±ƒüƒ±',
                'son_guncelleme': datetime.now().strftime('%Y-%m-%d'),
                'web_sitesi': item.get('web_sitesi', ''),
                'koordinat_lat': None,
                'koordinat_lon': None
            }
            
            # Coƒürafi e≈üleme uygula
            kurum = apply_geographic_mapping(kurum)
            
            if kurum['kurum_adi']:  # Bo≈ü isimleri filtrele
                kurum['kurum_id'] = generate_kurum_id(kurum)
                kurumlar.append(kurum)
                
    except FileNotFoundError:
        logger.warning("Saƒülƒ±k Bakanlƒ±ƒüƒ± veri dosyasƒ± bulunamadƒ±")
    except Exception as e:
        logger.error(f"Saƒülƒ±k Bakanlƒ±ƒüƒ± veri hatasƒ±: {e}")
    
    return kurumlar

def process_ozel_hastaneler_data() -> List[Dict[str, Any]]:
    """√ñzel hastane verilerini i≈üler."""
    kurumlar = []
    
    try:
        file_path = os.path.join('data', 'raw', 'ozel_hastaneler.json')
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        logger.info(f"√ñzel Hastaneler: {len(data)} kayƒ±t y√ºklendi")
        
        for item in data:
            kurum = {
                'kurum_adi': clean_text(item.get('kurum_adi', '')),
                'kurum_tipi': 'OZEL_HASTANE',
                'il_adi': clean_text(item.get('il_adi', '')),
                'ilce_adi': clean_text(item.get('ilce_adi', 'Merkez')),
                'adres': clean_address(item.get('adres', '')),
                'telefon': clean_phone_number(item.get('telefon', '')),
                'veri_kaynagi': '√ñzel Hastaneler Veritabanƒ±',
                'son_guncelleme': datetime.now().strftime('%Y-%m-%d'),
                'web_sitesi': item.get('web_sitesi', ''),
                'koordinat_lat': None,
                'koordinat_lon': None
            }
            
            # Coƒürafi e≈üleme uygula
            kurum = apply_geographic_mapping(kurum)
            
            if kurum['kurum_adi']:
                kurum['kurum_id'] = generate_kurum_id(kurum)
                kurumlar.append(kurum)
                
    except FileNotFoundError:
        logger.warning("√ñzel hastaneler veri dosyasƒ± bulunamadƒ±")
    except Exception as e:
        logger.error(f"√ñzel hastaneler veri hatasƒ±: {e}")
    
    return kurumlar

def process_universite_hastaneleri_data() -> List[Dict[str, Any]]:
    """√úniversite hastanesi verilerini i≈üler."""
    kurumlar = []
    
    try:
        file_path = os.path.join('data', 'raw', 'universite_hastaneleri.json')
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        logger.info(f"√úniversite Hastaneleri: {len(data)} kayƒ±t y√ºklendi")
        
        for item in data:
            kurum = {
                'kurum_adi': clean_text(item.get('kurum_adi', '')),
                'kurum_tipi': 'UNIVERSITE_HASTANESI',
                'il_adi': clean_text(item.get('il_adi', '')),
                'ilce_adi': clean_text(item.get('ilce_adi', 'Merkez')),
                'adres': clean_address(item.get('adres', '')),
                'telefon': clean_phone_number(item.get('telefon', '')),
                'veri_kaynagi': '√úniversite Hastaneleri Veritabanƒ±',
                'son_guncelleme': datetime.now().strftime('%Y-%m-%d'),
                'web_sitesi': item.get('web_sitesi', ''),
                'koordinat_lat': None,
                'koordinat_lon': None
            }
            
            # Coƒürafi e≈üleme uygula
            kurum = apply_geographic_mapping(kurum)
            
            if kurum['kurum_adi']:
                kurum['kurum_id'] = generate_kurum_id(kurum)
                kurumlar.append(kurum)
                
    except FileNotFoundError:
        logger.warning("√úniversite hastaneleri veri dosyasƒ± bulunamadƒ±")
    except Exception as e:
        logger.error(f"√úniversite hastaneleri veri hatasƒ±: {e}")
    
    return kurumlar

def remove_duplicates(kurumlar: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Dublicate kayƒ±tlarƒ± kaldƒ±rƒ±r."""
    seen = set()
    unique_kurumlar = []
    
    for kurum in kurumlar:
        # Anahtar olu≈ütur (isim + il + il√ße)
        key = (
            kurum.get('kurum_adi', '').lower().strip(),
            kurum.get('il_adi', '').lower().strip(),
            kurum.get('ilce_adi', '').lower().strip()
        )
        
        if key not in seen and key[0]:  # Bo≈ü isimleri dahil etme
            seen.add(key)
            unique_kurumlar.append(kurum)
    
    logger.info(f"Dublicate temizleme: {len(kurumlar)} -> {len(unique_kurumlar)}")
    return unique_kurumlar

def generate_statistics(kurumlar: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ƒ∞statistikler olu≈üturur."""
    iller = {}
    kurum_tipleri = {}
    
    for kurum in kurumlar:
        # ƒ∞l istatistikleri
        il = kurum.get('il_adi', 'Bilinmiyor')
        iller[il] = iller.get(il, 0) + 1
        
        # Kurum tipi istatistikleri
        tip = kurum.get('kurum_tipi', 'GENEL')
        kurum_tipleri[tip] = kurum_tipleri.get(tip, 0) + 1
    
    stats = {
        'toplam_kurum': len(kurumlar),
        'toplam_il': len(iller),
        'il_dagilimi': iller,
        'kurum_tipi_dagilimi': kurum_tipleri,
        'son_guncelleme': datetime.now().isoformat()
    }
    
    # ƒ∞statistik √ßƒ±ktƒ±sƒ±
    logger.info(f"Toplam Kurum: {stats['toplam_kurum']}")
    logger.info(f"Toplam ƒ∞l: {stats['toplam_il']}")
    
    if iller:
        en_cok_il = max(iller.keys(), key=lambda x: iller[x])
        logger.info(f"En √áok Kurum Olan ƒ∞l: {en_cok_il} ({iller[en_cok_il]})")
    
    return stats

def main():
    """Ana i≈ülem fonksiyonu."""
    logger.info("üè• T√úRKƒ∞YE SAƒûLIK KURULU≈ûLARI VERƒ∞ ƒ∞≈ûLEME")
    logger.info("üó∫Ô∏è T√ºrkiye'nin resmi 81 il sistemi ile coƒürafi e≈üleme")
    logger.info("=" * 60)
    
    # T√ºm veri kaynaklarƒ±nƒ± i≈üle
    all_kurumlar = []
    
    # Saƒülƒ±k Bakanlƒ±ƒüƒ± verileri
    sb_kurumlar = process_saglik_bakanligi_data()
    all_kurumlar.extend(sb_kurumlar)
    
    # √ñzel hastane verileri
    oh_kurumlar = process_ozel_hastaneler_data()
    all_kurumlar.extend(oh_kurumlar)
    
    # √úniversite hastane verileri
    uh_kurumlar = process_universite_hastaneleri_data()
    all_kurumlar.extend(uh_kurumlar)
    
    # Kapsamlƒ± √ºniversite-hastane ili≈ükileri
    from process_universite_hastane import UniversiteHastaneProcessor
    try:
        processor = UniversiteHastaneProcessor()
        kuh_kurumlar = processor.universite_hastane_iliskilerini_isle()
        all_kurumlar.extend(kuh_kurumlar)
        logger.info(f"‚úÖ {len(kuh_kurumlar)} kapsamlƒ± √ºniversite-hastane ili≈ükisi eklendi")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Kapsamlƒ± √ºniversite-hastane ili≈ükileri i≈ülenemedi: {e}")
    
    logger.info(f"Toplam ham veri: {len(all_kurumlar)} kurum")
    
    # Dublikasyonlarƒ± kaldƒ±r
    unique_kurumlar = remove_duplicates(all_kurumlar)
    
    # ƒ∞statistikler olu≈ütur
    stats = generate_statistics(unique_kurumlar)
    
    # Coƒürafi veriyi dƒ±≈üa aktar (mevcut fonksiyona sahipse)
    try:
        geo_mapper.export_geo_data('data/turkey_geo_data.json')
    except AttributeError:
        logger.info("Coƒürafi veri export fonksiyonu mevcut deƒüil, atlaniyor")
    
    # Ana veri yapƒ±sƒ±nƒ± olu≈ütur
    output_data = {
        'metadata': {
            'total_kurumlar': len(unique_kurumlar),
            'total_iller': 81,  # T√ºrkiye'nin resmi il sayƒ±sƒ±
            'veri_kaynaklari': [
                'T.C. Saƒülƒ±k Bakanlƒ±ƒüƒ±',
                '√ñzel Hastaneler Veritabanƒ±',
                '√úniversite Hastaneleri Veritabanƒ±',
                'Kapsamlƒ± √úniversite-Hastane ƒ∞li≈ükileri'
            ],
            'son_guncelleme': datetime.now().isoformat(),
            'geo_mapping_applied': True,
            'geo_system': 'Turkey Official 81 Provinces',
            'istatistikler': stats
        },
        'kurumlar': unique_kurumlar
    }
    
    # Ana dosyaya kaydet
    output_file = os.path.join('data', 'turkiye_saglik_kuruluslari.json')
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"‚úÖ Ana veri dosyasƒ± kaydedildi: {output_file}")
    logger.info(f"üó∫Ô∏è Coƒürafi veri kaydedildi: data/turkey_geo_data.json")
    logger.info(f"üìä {len(unique_kurumlar)} saƒülƒ±k kurumu i≈ülendi")
    logger.info(f"üèõÔ∏è T√ºrkiye'nin 81 ili standardƒ±nda coƒürafi e≈üleme tamamlandƒ±")
    
    # Coƒürafi e≈üle≈ütirme kalitesi analizi
    analyze_geographic_quality(output_data)
    
    return output_data

def analyze_geographic_quality(data: Dict) -> None:
    """Coƒürafi e≈üle≈ütirme kalitesini analiz et ve raporla"""
    kurumlar = data.get('kurumlar', [])
    total_kurumlar = len(kurumlar)
    
    # Ba≈üarƒ±lƒ± e≈üle≈üme analizi
    ba≈üarƒ±lƒ±_e≈üle≈üme = len([k for k in kurumlar if k.get('il_kodu') and 1 <= k.get('il_kodu', 0) <= 81])
    ba≈üarƒ±_oranƒ± = ba≈üarƒ±lƒ±_e≈üle≈üme / total_kurumlar * 100 if total_kurumlar else 0
    
    logger.info(f"üéØ Coƒürafi e≈üle≈ütirme ba≈üarƒ± oranƒ±: %{ba≈üarƒ±_oranƒ±:.1f} ({ba≈üarƒ±lƒ±_e≈üle≈üme}/{total_kurumlar})")
    
    if ba≈üarƒ±_oranƒ± >= 95:
        logger.info("üåü M√ºkemmel! Coƒürafi e≈üle≈ütirme sistemi √ßok ba≈üarƒ±lƒ±")
    elif ba≈üarƒ±_oranƒ± >= 90:
        logger.info("üëç ƒ∞yi! Coƒürafi e≈üle≈ütirme sistemi ba≈üarƒ±lƒ±")
    else:
        logger.warning("‚ö†Ô∏è Geli≈ütirilmeli! Coƒürafi e≈üle≈ütirme sisteminde iyile≈ütirme gerekli")
    
    # ƒ∞l daƒüƒ±lƒ±mƒ± kontrol
    il_sayƒ±sƒ± = len(set(k.get('il_adi') for k in kurumlar))
    if il_sayƒ±sƒ± == 81:
        logger.info("‚úÖ 81 il standardƒ±na uygun - T√ºm iller temsil ediliyor")
    else:
        logger.warning(f"‚ö†Ô∏è ƒ∞l sayƒ±sƒ± problemi: {il_sayƒ±sƒ±}/81")

if __name__ == "__main__":
    main()
