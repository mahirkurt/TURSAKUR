#!/usr/bin/env python3
"""
SaÄŸlÄ±k BakanlÄ±ÄŸÄ± Yeni Veri KaynaklarÄ± KeÅŸif Scripti
=================================================

Yeni keÅŸfedilen veri kaynaklarÄ±:
1. KHGM Kamu Hastaneleri Listesi (khgmsaglikhizmetleridb.saglik.gov.tr)
2. SaÄŸlÄ±k Turizmi Yetki Belgeli Tesisler (shgmturizmdb.saglik.gov.tr)
3. Ä°l SaÄŸlÄ±k MÃ¼dÃ¼rlÃ¼kleri (Ã–zel hekim muayenehaneleri)
4. Kalite Akreditasyon (shgmkalitedb.saglik.gov.tr)
"""

import requests
import json
import os
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
from bs4 import BeautifulSoup
import re

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SaglikBakanligiYeniKaynaklar:
    """SaÄŸlÄ±k BakanlÄ±ÄŸÄ±'nÄ±n yeni keÅŸfedilen veri kaynaklarÄ±nÄ± Ã§eken sÄ±nÄ±f."""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # Yeni keÅŸfedilen kaynaklar
        self.veri_kaynaklari = {
            'khgm_kamu_hastaneleri': {
                'url': 'https://khgmsaglikhizmetleridb.saglik.gov.tr/TR-87343/kamu-hastaneleri-genel-mudurlugune-bagli-2-ve-3-basamak-kamu-saglik-tesisleri-guncel-listesi.html',
                'tip': 'html_table',
                'hedef': 'Kamu hastaneleri 2.-3. basamak'
            },
            'saglik_turizmi': {
                'url': 'https://shgmturizmdb.saglik.gov.tr/',
                'tip': 'file_download',
                'hedef': 'SaÄŸlÄ±k turizmi yetki belgeli tesisler'
            },
            'khgm_site_haritasi': {
                'url': 'https://khgm.saglik.gov.tr/Siteagaci',
                'tip': 'link_crawler',
                'hedef': 'Åehir hastaneleri linkleri'
            },
            'ankara_il_saglik': {
                'url': 'https://ankaraism.saglik.gov.tr/TR-228957/dokumanlar.html',
                'tip': 'excel_download',
                'hedef': 'Ã–zel hekim muayenehaneleri'
            },
            'kalite_akreditasyon': {
                'url': 'https://shgmkalitedb.saglik.gov.tr/TR-52461/guncel-standartlar-excel-versiyon.html',
                'tip': 'excel_download',
                'hedef': 'Akredite saÄŸlÄ±k tesisleri'
            }
        }
        
        self.data_dir = os.path.join(os.path.dirname(__file__), '..', 'data', 'raw')
        os.makedirs(self.data_dir, exist_ok=True)
    
    def fetch_khgm_kamu_hastaneleri(self) -> List[Dict]:
        """KHGM Kamu hastaneleri listesini Ã§ek"""
        hospitals = []
        url = self.veri_kaynaklari['khgm_kamu_hastaneleri']['url']
        
        try:
            logger.info("KHGM Kamu hastaneleri listesi Ã§ekiliyor...")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Excel veya PDF linklerini bul
            links = soup.find_all('a', href=re.compile(r'\.(xlsx?|pdf)$', re.I))
            
            for link in links:
                href = link.get('href')
                text = link.get_text(strip=True)
                
                if 'liste' in text.lower() or 'excel' in text.lower():
                    if not href.startswith('http'):
                        href = 'https://khgmsaglikhizmetleridb.saglik.gov.tr' + href
                    
                    logger.info(f"âœ“ KHGM dosyasÄ± bulundu: {text} - {href}")
                    
                    # Bu Excel dosyasÄ±nÄ± indirip parse edebiliriz
                    if href.endswith(('.xlsx', '.xls')):
                        hospitals.extend(self._download_and_parse_excel(href, 'KHGM Kamu Hastaneleri'))
            
        except Exception as e:
            logger.error(f"KHGM kamu hastaneleri hatasÄ±: {e}")
        
        return hospitals
    
    def fetch_saglik_turizmi_tesisleri(self) -> List[Dict]:
        """SaÄŸlÄ±k turizmi yetki belgeli tesisleri Ã§ek"""
        facilities = []
        base_url = 'https://shgmturizmdb.saglik.gov.tr'
        
        try:
            logger.info("SaÄŸlÄ±k turizmi tesisleri Ã§ekiliyor...")
            response = self.session.get(base_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # PDF/Excel linklerini bul
            links = soup.find_all('a', href=re.compile(r'(yetki|liste|tesis)', re.I))
            
            for link in links:
                href = link.get('href')
                text = link.get_text(strip=True)
                
                if 'liste' in text.lower() or 'tesis' in text.lower():
                    if not href.startswith('http'):
                        href = base_url + href
                    
                    logger.info(f"âœ“ SaÄŸlÄ±k turizmi dosyasÄ±: {text} - {href}")
                    
                    # PDF'den metin Ã§Ä±karÄ±mÄ± veya Excel parse
                    if href.endswith('.pdf'):
                        facilities.extend(self._parse_pdf_content(href, 'SaÄŸlÄ±k Turizmi Tesisi'))
                    elif href.endswith(('.xlsx', '.xls')):
                        facilities.extend(self._download_and_parse_excel(href, 'SaÄŸlÄ±k Turizmi Tesisi'))
            
        except Exception as e:
            logger.error(f"SaÄŸlÄ±k turizmi tesisleri hatasÄ±: {e}")
        
        return facilities
    
    def fetch_il_saglik_mudurlukteri(self) -> List[Dict]:
        """81 Ä°l SaÄŸlÄ±k MÃ¼dÃ¼rlÃ¼ÄŸÃ¼'nden Ã¶zel hekim listelerini Ã§ek"""
        all_facilities = []
        
        # BÃ¼yÃ¼k ÅŸehir il saÄŸlÄ±k mÃ¼dÃ¼rlÃ¼kleri
        il_domains = [
            'ankaraism.saglik.gov.tr',
            'istanbulism.saglik.gov.tr', 
            'izmirism.saglik.gov.tr',
            'bursaism.saglik.gov.tr',
            'antalyaism.saglik.gov.tr',
            'adanaism.saglik.gov.tr',
            'konyaism.saglik.gov.tr',
            'gaziantepism.saglik.gov.tr',
            'kayserism.saglik.gov.tr'
        ]
        
        for domain in il_domains:
            try:
                logger.info(f"Ä°l SaÄŸlÄ±k MÃ¼dÃ¼rlÃ¼ÄŸÃ¼ taranÄ±yor: {domain}")
                
                # Dokuman sayfasÄ±nÄ± kontrol et
                urls_to_check = [
                    f'https://{domain}/TR-228957/dokumanlar.html',
                    f'https://{domain}/dokumanlar',
                    f'https://{domain}/ozel-hekim',
                    f'https://{domain}/listeler'
                ]
                
                for url in urls_to_check:
                    facilities = self._fetch_il_documents(url, domain.split('.')[0])
                    all_facilities.extend(facilities)
                    
            except Exception as e:
                logger.warning(f"Ä°l SaÄŸlÄ±k MÃ¼dÃ¼rlÃ¼ÄŸÃ¼ hatasÄ± {domain}: {e}")
                continue
        
        return all_facilities
    
    def _download_and_parse_excel(self, url: str, facility_type: str) -> List[Dict]:
        """Excel dosyasÄ±nÄ± indir ve parse et"""
        facilities = []
        
        try:
            logger.info(f"Excel indiriliyor: {url}")
            response = self.session.get(url, timeout=60)
            response.raise_for_status()
            
            # GeÃ§ici dosya olarak kaydet
            temp_file = f"temp_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
            with open(temp_file, 'wb') as f:
                f.write(response.content)
            
            # Pandas ile parse et
            try:
                import pandas as pd
                df = pd.read_excel(temp_file, sheet_name=0)
                
                for _, row in df.iterrows():
                    facility = {
                        'kurum_adi': str(row.iloc[0]) if len(row) > 0 else '',
                        'kurum_tipi': facility_type,
                        'veri_kaynagi': f'SaÄŸlÄ±k BakanlÄ±ÄŸÄ± - {facility_type}',
                        'son_guncelleme': datetime.now().strftime('%Y-%m-%d'),
                        'excel_url': url
                    }
                    
                    # DiÄŸer sÃ¼tunlarÄ± da ekle
                    if len(row) > 1:
                        facility['il_adi'] = str(row.iloc[1])
                    if len(row) > 2:
                        facility['ilce_adi'] = str(row.iloc[2])
                    if len(row) > 3:
                        facility['adres'] = str(row.iloc[3])
                    
                    facilities.append(facility)
                
                logger.info(f"âœ… Excel parse edildi: {len(facilities)} kayÄ±t")
                
            except Exception as parse_error:
                logger.error(f"Excel parse hatasÄ±: {parse_error}")
            
            # GeÃ§ici dosyayÄ± sil
            if os.path.exists(temp_file):
                os.remove(temp_file)
                
        except Exception as e:
            logger.error(f"Excel indirme hatasÄ± {url}: {e}")
        
        return facilities
    
    def _parse_pdf_content(self, url: str, facility_type: str) -> List[Dict]:
        """PDF iÃ§eriÄŸini parse et (basit metin Ã§Ä±karÄ±mÄ±)"""
        facilities = []
        
        try:
            logger.info(f"PDF indiriliyor: {url}")
            response = self.session.get(url, timeout=60)
            response.raise_for_status()
            
            # PDF parse iÃ§in PyPDF2 veya pdfplumber kullanÄ±labilir
            # Åimdilik basit text search yapalÄ±m
            
            temp_file = f"temp_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
            with open(temp_file, 'wb') as f:
                f.write(response.content)
            
            # PDF text extraction burada olacak
            logger.info(f"PDF kaydedildi: {temp_file} (manuel parse gerekli)")
            
            # GeÃ§ici dosyayÄ± temizle
            if os.path.exists(temp_file):
                os.remove(temp_file)
                
        except Exception as e:
            logger.error(f"PDF parse hatasÄ± {url}: {e}")
        
        return facilities
    
    def _fetch_il_documents(self, url: str, il_adi: str) -> List[Dict]:
        """Ä°l SaÄŸlÄ±k MÃ¼dÃ¼rlÃ¼ÄŸÃ¼ dokÃ¼man sayfasÄ±nÄ± tara"""
        facilities = []
        
        try:
            response = self.session.get(url, timeout=20)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Excel linklerini bul
            excel_links = soup.find_all('a', href=re.compile(r'\.(xlsx?|csv)$', re.I))
            
            for link in excel_links:
                href = link.get('href')
                text = link.get_text(strip=True)
                
                if any(keyword in text.lower() for keyword in ['hekim', 'muayene', 'liste', 'tesis']):
                    if not href.startswith('http'):
                        base_url = '/'.join(url.split('/')[:3])
                        href = base_url + href
                    
                    logger.info(f"âœ“ {il_adi} Excel bulundu: {text}")
                    facilities.extend(self._download_and_parse_excel(href, f'{il_adi} Ã–zel Hekim'))
            
        except Exception as e:
            logger.warning(f"Ä°l dokÃ¼man sayfasÄ± hatasÄ± {url}: {e}")
        
        return facilities
    
    def fetch_all_new_sources(self) -> List[Dict]:
        """TÃ¼m yeni veri kaynaklarÄ±nÄ± Ã§ek"""
        all_data = []
        
        logger.info("ğŸš€ SaÄŸlÄ±k BakanlÄ±ÄŸÄ± yeni veri kaynaklarÄ± taranÄ±yor...")
        
        # 1. KHGM Kamu Hastaneleri
        khgm_data = self.fetch_khgm_kamu_hastaneleri()
        all_data.extend(khgm_data)
        logger.info(f"âœ… KHGM: {len(khgm_data)} kayÄ±t")
        
        # 2. SaÄŸlÄ±k Turizmi Tesisleri
        turizm_data = self.fetch_saglik_turizmi_tesisleri()
        all_data.extend(turizm_data)
        logger.info(f"âœ… SaÄŸlÄ±k Turizmi: {len(turizm_data)} kayÄ±t")
        
        # 3. Ä°l SaÄŸlÄ±k MÃ¼dÃ¼rlÃ¼kleri
        il_data = self.fetch_il_saglik_mudurlukteri()
        all_data.extend(il_data)
        logger.info(f"âœ… Ä°l SaÄŸlÄ±k MÃ¼d.: {len(il_data)} kayÄ±t")
        
        logger.info(f"ğŸ¯ Toplam yeni veri: {len(all_data)} kayÄ±t")
        return all_data
    
    def save_data(self, data: List[Dict]):
        """Verileri kaydet"""
        json_file = os.path.join(self.data_dir, 'saglik_bakanligi_yeni_kaynaklar.json')
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ Veriler kaydedildi: {json_file}")

def main():
    """Ana fonksiyon"""
    logger.info("SaÄŸlÄ±k BakanlÄ±ÄŸÄ± yeni veri kaynaklarÄ± keÅŸfi baÅŸlÄ±yor...")
    
    fetcher = SaglikBakanligiYeniKaynaklar()
    
    try:
        all_data = fetcher.fetch_all_new_sources()
        
        if all_data:
            fetcher.save_data(all_data)
            logger.info("âœ… SaÄŸlÄ±k BakanlÄ±ÄŸÄ± yeni kaynaklar iÅŸlemi tamamlandÄ±!")
        else:
            logger.warning("âŒ HiÃ§ yeni veri bulunamadÄ±!")
            
    except Exception as e:
        logger.error(f"âŒ Ä°ÅŸlem baÅŸarÄ±sÄ±z: {e}")

if __name__ == "__main__":
    main()
